{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import torch\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from groq import Groq\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from rouge_score import rouge_scorer\n",
    "load_dotenv()\n",
    "os.environ['GROQ_API_KEY'] = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>which person directed the movies starred by [J...</td>\n",
       "      <td>Nancy Meyers|Sam Mendes|George Clooney|Ken Kwa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>who are movie co-directors of [Delbert Mann]</td>\n",
       "      <td>Franco Zeffirelli|Cary Fukunaga|Lewis Mileston...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>what are the primary languages in the movies d...</td>\n",
       "      <td>German</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the screenwriter [Mimsy Farmer] co-wrote movie...</td>\n",
       "      <td>Barbet Schroeder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the films acted by [Shaun White] were in which...</td>\n",
       "      <td>Sport|Documentary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14867</th>\n",
       "      <td>who are the actors in the movies directed by [...</td>\n",
       "      <td>Gary Sinise|Debra Messing|Ashton Kutcher|Marti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14868</th>\n",
       "      <td>the films directed by [Larry Charles] were rel...</td>\n",
       "      <td>2003|2009|2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14869</th>\n",
       "      <td>the director [Frank Oz] co-directed films with...</td>\n",
       "      <td>Neil LaBute|Jim Henson|Bryan Forbes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14870</th>\n",
       "      <td>what were the release dates of [Ritwik Ghatak]...</td>\n",
       "      <td>1958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14871</th>\n",
       "      <td>which person wrote the movies starred by [Paul...</td>\n",
       "      <td>Alun Owen</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14872 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                question  \\\n",
       "0      which person directed the movies starred by [J...   \n",
       "1           who are movie co-directors of [Delbert Mann]   \n",
       "2      what are the primary languages in the movies d...   \n",
       "3      the screenwriter [Mimsy Farmer] co-wrote movie...   \n",
       "4      the films acted by [Shaun White] were in which...   \n",
       "...                                                  ...   \n",
       "14867  who are the actors in the movies directed by [...   \n",
       "14868  the films directed by [Larry Charles] were rel...   \n",
       "14869  the director [Frank Oz] co-directed films with...   \n",
       "14870  what were the release dates of [Ritwik Ghatak]...   \n",
       "14871  which person wrote the movies starred by [Paul...   \n",
       "\n",
       "                                                  answer  \n",
       "0      Nancy Meyers|Sam Mendes|George Clooney|Ken Kwa...  \n",
       "1      Franco Zeffirelli|Cary Fukunaga|Lewis Mileston...  \n",
       "2                                                 German  \n",
       "3                                       Barbet Schroeder  \n",
       "4                                      Sport|Documentary  \n",
       "...                                                  ...  \n",
       "14867  Gary Sinise|Debra Messing|Ashton Kutcher|Marti...  \n",
       "14868                                     2003|2009|2012  \n",
       "14869                Neil LaBute|Jim Henson|Bryan Forbes  \n",
       "14870                                               1958  \n",
       "14871                                          Alun Owen  \n",
       "\n",
       "[14872 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df_2hop = pd.read_csv('../dataset/MetaQA/2-hop/qa_test.txt', sep='\\t', header=None, names=['question', 'answer'])\n",
    "test_df_2hop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieKnowledgeBase:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = 'sentence-transformers/all-MiniLM-L6-v2',\n",
    "        gpu_id: int = 0,\n",
    "        save_dir: str = './knowledge_base'\n",
    "    ):\n",
    "        self.save_dir = Path(save_dir)\n",
    "        self.save_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        self.device = f'cuda:{gpu_id}' if torch.cuda.is_available() else 'cpu'\n",
    "        self.model = SentenceTransformer(model_name, device=self.device)\n",
    "        self.embedding_size = self.model.get_sentence_embedding_dimension()\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            self.gpu_resources = faiss.StandardGpuResources()\n",
    "            self.cpu_index = faiss.IndexFlatIP(self.embedding_size)\n",
    "            self.index = faiss.index_cpu_to_gpu(\n",
    "                self.gpu_resources, gpu_id, self.cpu_index\n",
    "            )\n",
    "        else:\n",
    "            self.index = faiss.IndexFlatIP(self.embedding_size)\n",
    "        \n",
    "        self.movie_facts = []\n",
    "        self.fact_texts = []\n",
    "        \n",
    "    def _process_kb_line(self, line: str) -> Dict:\n",
    "        \"\"\"Process a single line from the knowledge base file.\"\"\"\n",
    "        movie, relation, value = line.strip().split('|')\n",
    "        return {\n",
    "            'movie': movie,\n",
    "            'relation': relation,\n",
    "            'value': value\n",
    "        }\n",
    "    \n",
    "    def _create_fact_text(self, fact: Dict) -> str:\n",
    "        \"\"\"Create a natural language representation of a fact.\"\"\"\n",
    "\n",
    "        relation_templates = {\n",
    "            'directed_by': '{movie} was directed by {value}',\n",
    "            'written_by': '{movie} was written by {value}',\n",
    "            'starred_actors': '{value} starred in {movie}',\n",
    "            'release_year': '{movie} was released in {value}',\n",
    "            'in_language': '{movie} is in {value}',\n",
    "            'has_genre': '{movie} is a {value} movie',\n",
    "            'has_tags': '{movie} has tag: {value}',\n",
    "            'has_imdb_votes': '{movie} has {value} IMDb votes'\n",
    "        }\n",
    "        \n",
    "        template = relation_templates.get(\n",
    "            fact['relation'],\n",
    "            '{movie} {relation} {value}'\n",
    "        )\n",
    "        \n",
    "        return template.format(**fact)\n",
    "    \n",
    "    def load_knowledge_base(self, kb_file: str):\n",
    "        \"\"\"Load and process the knowledge base file.\"\"\"\n",
    "        print(\"Loading knowledge base...\")\n",
    "        with open(kb_file, 'r', encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "        \n",
    "        for line in lines:\n",
    "            fact = self._process_kb_line(line)\n",
    "            self.movie_facts.append(fact)\n",
    "            fact_text = self._create_fact_text(fact)\n",
    "            self.fact_texts.append(fact_text)\n",
    "        \n",
    "        print(\"Creating embeddings...\")\n",
    "        embeddings = self.model.encode(\n",
    "            self.fact_texts,\n",
    "            convert_to_numpy=True,\n",
    "            normalize_embeddings=True,\n",
    "            batch_size=32\n",
    "        )\n",
    "        \n",
    "        self.index.add(embeddings)\n",
    "        print(f\"Added {len(self.fact_texts)} facts to the index\")\n",
    "        \n",
    "        self.save_knowledge_base()\n",
    "    \n",
    "    def save_knowledge_base(self):\n",
    "        \"\"\"Save the processed knowledge base and index.\"\"\"\n",
    "        print(\"Saving knowledge base...\")\n",
    "        \n",
    "        with open(self.save_dir / 'fact_texts.pkl', 'wb') as f:\n",
    "            pickle.dump(self.fact_texts, f)\n",
    "        \n",
    "        with open(self.save_dir / 'movie_facts.pkl', 'wb') as f:\n",
    "            pickle.dump(self.movie_facts, f)\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            cpu_index = faiss.index_gpu_to_cpu(self.index)\n",
    "        else:\n",
    "            cpu_index = self.index\n",
    "            \n",
    "        faiss.write_index(cpu_index, str(self.save_dir / 'faiss_index.bin'))\n",
    "        print(\"Knowledge base saved successfully\")\n",
    "    \n",
    "    def load_saved_knowledge_base(self):\n",
    "        \"\"\"Load a previously saved knowledge base.\"\"\"\n",
    "        print(\"Loading saved knowledge base...\")\n",
    "        \n",
    "        with open(self.save_dir / 'fact_texts.pkl', 'rb') as f:\n",
    "            self.fact_texts = pickle.load(f)\n",
    "        \n",
    "        with open(self.save_dir / 'movie_facts.pkl', 'rb') as f:\n",
    "            self.movie_facts = pickle.load(f)\n",
    "        \n",
    "        cpu_index = faiss.read_index(str(self.save_dir / 'faiss_index.bin'))\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            self.index = faiss.index_cpu_to_gpu(\n",
    "                self.gpu_resources, 0, cpu_index\n",
    "            )\n",
    "        else:\n",
    "            self.index = cpu_index\n",
    "            \n",
    "        print(\"Knowledge base loaded successfully\")\n",
    "    \n",
    "    def retrieve_context(self, question: str, k: int = 5) -> List[str]:\n",
    "        \"\"\"Retrieve relevant context for a question.\"\"\"\n",
    "\n",
    "        question_embedding = self.model.encode(\n",
    "            question,\n",
    "            convert_to_numpy=True,\n",
    "            normalize_embeddings=True\n",
    "        )\n",
    "        \n",
    "        scores, indices = self.index.search(\n",
    "            question_embedding.reshape(1, -1),\n",
    "            k\n",
    "        )\n",
    "        \n",
    "        relevant_facts = [self.fact_texts[idx] for idx in indices[0]]\n",
    "        return relevant_facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_step_retrieve(kb: MovieKnowledgeBase, question: str, k_array: List[int]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Perform multi-step retrieval, accounting for self-retrieval in subsequent steps.\n",
    "    \"\"\"\n",
    "    all_retrieved = set()  # Use set to avoid duplicates\n",
    "    current_level_queries = [question]  # Start with original question\n",
    "    \n",
    "    for step, k in enumerate(k_array):\n",
    "        next_level_queries = []\n",
    "        \n",
    "        # For each query at this step\n",
    "        for query in current_level_queries:\n",
    "            # Get embeddings and search\n",
    "            query_embedding = kb.model.encode(\n",
    "                query,\n",
    "                convert_to_numpy=True,\n",
    "                normalize_embeddings=True\n",
    "            )\n",
    "            \n",
    "            # For subsequent steps (after first), retrieve k+1 and skip first result\n",
    "            if step == 0:\n",
    "                num_to_retrieve = k\n",
    "                start_idx = 0\n",
    "            else:\n",
    "                num_to_retrieve = k + 1\n",
    "                start_idx = 1  # Skip the first result (self-retrieval)\n",
    "            \n",
    "            scores, indices = kb.index.search(\n",
    "                query_embedding.reshape(1, -1),\n",
    "                num_to_retrieve\n",
    "            )\n",
    "            \n",
    "            # Get retrieved facts for this query, skipping self-retrieval if necessary\n",
    "            retrieved_facts = [kb.fact_texts[idx] for idx in indices[0][start_idx:]]\n",
    "            \n",
    "            # Add to overall set\n",
    "            all_retrieved.update(retrieved_facts)\n",
    "            \n",
    "            # Add to next level queries\n",
    "            next_level_queries.extend(retrieved_facts)\n",
    "        \n",
    "        # Update current level queries for next iteration\n",
    "        current_level_queries = next_level_queries\n",
    "        \n",
    "        if not current_level_queries or step == len(k_array) - 1:\n",
    "            break\n",
    "    \n",
    "    retrieved_list = list(all_retrieved)\n",
    "    print(f\"\\nTotal unique contexts retrieved: {len(retrieved_list)}\")\n",
    "    print(f\"Expected total (max possible unique): {sum([k_array[0] * (k_array[1] ** i) for i in range(len(k_array))])}\")\n",
    "    return retrieved_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_qa_dataset(\n",
    "    kb: MovieKnowledgeBase,\n",
    "    qa_df: pd.DataFrame,\n",
    "    llm_call_function,\n",
    "    k_array: List[int] = [3, 2]  # Example: First get top 3, then top 2 for each\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Process QA dataset with multi-step retrieval knowledge base context.\n",
    "    \n",
    "    Args:\n",
    "        kb: MovieKnowledgeBase instance\n",
    "        qa_df: DataFrame with 'question' and 'answer' columns\n",
    "        llm_call_function: Function that makes API call to LLM\n",
    "        k_array: Array specifying number of retrievals at each step\n",
    "    \n",
    "    Returns:\n",
    "        List of LLM responses\n",
    "    \"\"\"\n",
    "    responses = []\n",
    "    \n",
    "    for idx, row in qa_df.iterrows():\n",
    "        question = row['question']\n",
    "        ground_truth = row['answer']\n",
    "        \n",
    "        # Debug print for this question\n",
    "        print(f\"\\nProcessing Question: {question}\")\n",
    "        \n",
    "        # Perform multi-step retrieval\n",
    "        retrieved_contexts = multi_step_retrieve(kb, question, k_array)\n",
    "        \n",
    "        # Debug prints for retrieval results\n",
    "        print(\"\\nRetrieval breakdown:\")\n",
    "        print(f\"Expected total contexts: {sum([k_array[0] * (k_array[1] ** i) for i in range(len(k_array))])}\")\n",
    "        print(f\"Actual unique contexts retrieved: {len(retrieved_contexts)}\")\n",
    "        print(\"\\nRetrieved contexts:\")\n",
    "        for i, context in enumerate(retrieved_contexts, 1):\n",
    "            print(f\"{i}. {context}\")\n",
    "        \n",
    "        context_text = \"\\n\".join(retrieved_contexts)\n",
    "        \n",
    "        prompt = f\"\"\"Based on the following context, please answer the question.\n",
    "        \n",
    "Context:\n",
    "{context_text}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer: \"\"\"\n",
    "        \n",
    "        response = llm_call_function(prompt)\n",
    "        print('\\nResults:')\n",
    "        print('Question: ', question)\n",
    "        print('Ground Truth: ', ground_truth)\n",
    "        print('Response: ', response)\n",
    "        print(f'Total unique contexts used: {len(retrieved_contexts)}')\n",
    "        print('-' * 50)\n",
    "        \n",
    "        # Write to a txt file\n",
    "        with open('qa_results.txt', 'a') as f:\n",
    "            f.write(f'Question: {question}\\n')\n",
    "            f.write(f'Ground Truth: {ground_truth}\\n')\n",
    "            f.write('Retrieved Contexts:\\n')\n",
    "            for i, context in enumerate(retrieved_contexts, 1):\n",
    "                f.write(f\"{i}. {context}\\n\")\n",
    "            f.write(f'Response: {response}\\n')\n",
    "            f.write(f'Total unique contexts: {len(retrieved_contexts)}\\n')\n",
    "            f.write('-' * 50 + '\\n\\n')\n",
    "        \n",
    "        responses.append(response)\n",
    "    \n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kb = MovieKnowledgeBase(save_dir='./movie_kb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading knowledge base...\n",
      "Creating embeddings...\n",
      "Added 134741 facts to the index\n",
      "Saving knowledge base...\n",
      "Knowledge base saved successfully\n"
     ]
    }
   ],
   "source": [
    "kb.load_knowledge_base('../dataset/MetaQA/kb.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rouge_score(hypothesis: str, reference: str) -> Dict:\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "    scores = scorer.score(reference, hypothesis)\n",
    "    return {\n",
    "        'rouge1_f1': scores['rouge1'].fmeasure,\n",
    "        'rouge1_precision': scores['rouge1'].precision,\n",
    "        'rouge1_recall': scores['rouge1'].recall,\n",
    "        'rougeL_f1': scores['rougeL'].fmeasure,\n",
    "        'rougeL_precision': scores['rougeL'].precision,\n",
    "        'rougeL_recall': scores['rougeL'].recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Groq(\n",
    "    api_key=os.environ.get(\"GROQ_API_KEY\"),\n",
    ")\n",
    "\n",
    "def lm_call(prompt: str) -> str:\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"\"\"\n",
    "Rules:\n",
    "- Answer ONLY using the information provided in the context\n",
    "- Provide ONLY the answer, with no explanations or additional text\n",
    "- Keep answers concise and to the point\n",
    "- If there are multiple answers, output them in a pipe-separated list (e.g. \"answer1|answer2\")\n",
    "\"\"\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt,\n",
    "        }\n",
    "    ],\n",
    "        temperature=0.7,\n",
    "        model=\"llama-3.2-3b-preview\",\n",
    "    )\n",
    "\n",
    "    return chat_completion.choices[0].message.content\n",
    "\n",
    "responses = process_qa_dataset(kb, test_df_2hop, lm_call)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for q, a, r in zip(test_df_2hop['question'], test_df_2hop['answer'], responses):\n",
    "    print(f\"\\nQuestion: {q}\")\n",
    "    print(f\"Ground Truth: {a}\")\n",
    "    print(f\"LLM Response: {r}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
