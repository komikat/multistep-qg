{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_model import load_model, generate_text\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from groq import Groq\n",
    "import json\n",
    "import os\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = \"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\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer, model = load_model()\n",
    "rag_model = SentenceTransformer('all-MiniLM-L6-v2') # all-mpnet-base-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>paragraphs</th>\n",
       "      <th>question</th>\n",
       "      <th>question_decomposition</th>\n",
       "      <th>answer</th>\n",
       "      <th>answer_aliases</th>\n",
       "      <th>answerable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2hop__460946_294723</td>\n",
       "      <td>[{'idx': 0, 'title': 'Grant's First Stand', 'p...</td>\n",
       "      <td>Who is the spouse of the Green performer?</td>\n",
       "      <td>[{'id': 460946, 'question': 'Green &gt;&gt; performe...</td>\n",
       "      <td>Miquette Giraudy</td>\n",
       "      <td>[]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2hop__252311_366220</td>\n",
       "      <td>[{'idx': 0, 'title': 'SICRAL 1B', 'paragraph_t...</td>\n",
       "      <td>Who founded the company that distributed the f...</td>\n",
       "      <td>[{'id': 252311, 'question': 'UHF &gt;&gt; distribute...</td>\n",
       "      <td>Mike Medavoy</td>\n",
       "      <td>[]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2hop__701895_752697</td>\n",
       "      <td>[{'idx': 0, 'title': 'Pangi Territory', 'parag...</td>\n",
       "      <td>What administrative territorial entity is the ...</td>\n",
       "      <td>[{'id': 701895, 'question': 'Ciudad Deportiva ...</td>\n",
       "      <td>Tamaulipas</td>\n",
       "      <td>[]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2hop__259228_793698</td>\n",
       "      <td>[{'idx': 0, 'title': 'Richmond, Virginia', 'pa...</td>\n",
       "      <td>Where is Ulrich Walter's employer headquartered?</td>\n",
       "      <td>[{'id': 259228, 'question': 'Ulrich Walter &gt;&gt; ...</td>\n",
       "      <td>Cologne</td>\n",
       "      <td>[]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2hop__481349_302087</td>\n",
       "      <td>[{'idx': 0, 'title': 'Market Square (High Poin...</td>\n",
       "      <td>Which company owns the manufacturer of Learjet...</td>\n",
       "      <td>[{'id': 481349, 'question': 'Learjet 60 &gt;&gt; man...</td>\n",
       "      <td>Bombardier Inc.</td>\n",
       "      <td>[Bombardier]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id                                         paragraphs  \\\n",
       "0  2hop__460946_294723  [{'idx': 0, 'title': 'Grant's First Stand', 'p...   \n",
       "1  2hop__252311_366220  [{'idx': 0, 'title': 'SICRAL 1B', 'paragraph_t...   \n",
       "2  2hop__701895_752697  [{'idx': 0, 'title': 'Pangi Territory', 'parag...   \n",
       "3  2hop__259228_793698  [{'idx': 0, 'title': 'Richmond, Virginia', 'pa...   \n",
       "4  2hop__481349_302087  [{'idx': 0, 'title': 'Market Square (High Poin...   \n",
       "\n",
       "                                            question  \\\n",
       "0          Who is the spouse of the Green performer?   \n",
       "1  Who founded the company that distributed the f...   \n",
       "2  What administrative territorial entity is the ...   \n",
       "3   Where is Ulrich Walter's employer headquartered?   \n",
       "4  Which company owns the manufacturer of Learjet...   \n",
       "\n",
       "                              question_decomposition            answer  \\\n",
       "0  [{'id': 460946, 'question': 'Green >> performe...  Miquette Giraudy   \n",
       "1  [{'id': 252311, 'question': 'UHF >> distribute...      Mike Medavoy   \n",
       "2  [{'id': 701895, 'question': 'Ciudad Deportiva ...        Tamaulipas   \n",
       "3  [{'id': 259228, 'question': 'Ulrich Walter >> ...           Cologne   \n",
       "4  [{'id': 481349, 'question': 'Learjet 60 >> man...   Bombardier Inc.   \n",
       "\n",
       "  answer_aliases  answerable  \n",
       "0             []        True  \n",
       "1             []        True  \n",
       "2             []        True  \n",
       "3             []        True  \n",
       "4   [Bombardier]        True  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df = pd.read_json(\"val.json\", lines=True)\n",
    "val_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2417, 7)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 460946,\n",
       "  'question': 'Green >> performer',\n",
       "  'answer': 'Steve Hillage',\n",
       "  'paragraph_support_idx': 10},\n",
       " {'id': 294723,\n",
       "  'question': '#1 >> spouse',\n",
       "  'answer': 'Miquette Giraudy',\n",
       "  'paragraph_support_idx': 5}]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df['question_decomposition'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement RAG to find the closest paragraphs to the question\n",
    "def get_closest_paragraphs(question, paragraphs, top_k=5, second_closest=False):\n",
    "    # print(\"<<<<<\", paragraphs)\n",
    "    question_embedding = rag_model.encode(question)\n",
    "    paragraph_embeddings = [rag_model.encode(paragraph) for paragraph in paragraphs] \n",
    "    similarity = [np.dot(question_embedding, paragraph_embedding) / (np.linalg.norm(question_embedding) * np.linalg.norm(paragraph_embedding)) for paragraph_embedding in paragraph_embeddings]\n",
    "    \n",
    "    # return the topk paragraphs\n",
    "    if second_closest:\n",
    "        topk_idx = np.argsort(similarity)[-top_k-1:-1].tolist()\n",
    "    else:\n",
    "        topk_idx = np.argsort(similarity)[-top_k:].tolist() # in this line, we are sorting the similarity scores and getting the topk indexes\n",
    "    topk_sentences = [paragraphs[i] for i in topk_idx]\n",
    "    # print(topk_idx)\n",
    "    return topk_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# client = Groq()\n",
    "\n",
    "# chat_completion = client.chat.completions.create(\n",
    "\n",
    "#     messages=[\n",
    "#         {\n",
    "#             \"role\": \"system\",\n",
    "#             \"content\": \"you are a helpful assistant.\"\n",
    "#         },\n",
    "#         # Set a user message for the assistant to respond to.\n",
    "#         {\n",
    "#             \"role\": \"user\",\n",
    "#             \"content\": \"Explain the importance of fast language models\",\n",
    "#         }\n",
    "#     ],\n",
    "\n",
    "#     # The language model which will generate the completion.\n",
    "#     model=\"llama-3.2-3b-preview\",\n",
    "#     temperature=0.5,\n",
    "#     max_tokens=8192,\n",
    "#     top_p=1,\n",
    "\n",
    "#     # A stop sequence is a predefined or user-specified text string that\n",
    "#     # signals an AI to stop generating content, ensuring its responses\n",
    "#     # remain focused and concise. Examples include punctuation marks and\n",
    "#     # markers like \"[end]\".\n",
    "#     stop=None,\n",
    "\n",
    "#     # If set, partial message deltas will be sent.\n",
    "#     stream=False,\n",
    "# )\n",
    "\n",
    "# # Print the completion returned by the LLM.\n",
    "# print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reponse\t\tGround Truth\n",
      "Not specified in context\t\tMiquette Giraudy\n",
      "Richard O'Brien\t\tMike Medavoy\n",
      "The municipality of Nuevo Laredo\t\tTamaulipas\n",
      "I couldn't find information\t\tCologne\n",
      "Bombardier Inc owns Bomb\t\tBombardier Inc\n",
      "Daniel Webster's child\t\tFletcher Webster\n",
      "Anne of Austria\t\tMarie de' Medici\n",
      "Promoting European integration\t\tEuropean integration\n",
      "Adelphia Communications failed\t\tTime Warner Cable\n",
      "Cabo Delgado Province\t\tCabo Delgado Province\n",
      "Benevento Calcio plays Serie\t\tLega Pro Prima Divisione\n",
      "'The Celestine Prophecy\t\tAnderson Tapes\n",
      "I couldn't find information about Call\t\tKnowsley\n",
      "Pecos County shares border with\t\tCrockett County\n",
      "Holley, New York County\t\tOrleans County\n",
      "No specific award mentioned\t\tAcademy Award for Best Animated Short Film\n",
      "Blair International Baccalaureate\t\tBlair High School\n",
      "Nicole Kidman's spouse\t\tKeith Urban\n",
      "King Gustav Vasa of Sweden\t\tSvante Stensson Sture\n",
      "Randall County, Texas\t\tRandall County\n",
      "Universal Music Japan\t\tDerrty Entertainment\n",
      "Not mentioned in the context\t\tExeter College\n",
      "Charles County, Maryland\t\tCharles County\n",
      "There is no information provided\t\tChynna Phillips\n",
      "Southwest City, Missouri\t\tMcDonald County\n",
      "Asian Man Records started\t\tAsian Man Records\n",
      "Emilia Galotti is another notable work\t\tEmilia Galotti\n",
      "Grover Washington\t\tconga\n",
      "Stanley, North Dakota\t\tStanley\n",
      "Henry Baring of Cromer Hall\t\tSir Francis Baring, 1st Baronet\n",
      "The Eagles music group\t\tJackson 5\n",
      "Eddie Argos is the lead singer\t\tEddie Argos\n",
      "The National Cycle Network\t\tnational cycling route network\n",
      "Johan Ludvig Heiberg\t\tJohanne Luise Heiberg\n",
      "The Peace Corps\t\tPeace Corps\n",
      "Van Horn, Texas\t\tVan Horn\n",
      "Corfe Mullen in Dorset\t\tEast Dorset\n",
      "Erikson's wife, Joan Erik\t\tJoan Erikson\n",
      "Sundazed Records\t\tColumbia Records\n",
      "Stefan Nemanja's child\t\tSaint Sava\n",
      "Pittsburgh Pirates baseball team\t\tPittsburgh Pirates\n",
      "Central Equatoria shares border with\t\tEastern Equatoria\n",
      "Amherst, Nova Scotia\t\tAmherst\n",
      "Patabendi Don Jinadasa\t\tRohana Wijeweera\n",
      "Hastings County\t\tHaliburton County\n",
      "Hancock County, Indiana\t\tHancock County\n",
      "Lorenzo de' Medici\t\tGiuliano de' Medici\n",
      "Waseca County, Minnesota\t\tWaseca County\n",
      "The Maltese football league\t\tMaltese Premier League\n",
      "Leslie Perrins' wife was unknown\t\tJohn Profumo\n",
      "Park Rapids, Minnesota\t\tHubbard County\n",
      "Annapolis, Maryland\t\tAnnapolis\n",
      "Def Squad members\t\tErick Sermon\n",
      "Bohol Island\t\tPanglao Island\n",
      "Fort Mitchell, Kentucky county\t\tKenton County\n",
      "Kingscliff, New South Wales\t\tKingscliff\n",
      "Not mentioned in the text\t\tWest Berkshire\n",
      "Eldon, Missouri, is not\t\tMiller County\n",
      "Ruth Gordon was spouse\t\tGarson Kanin\n",
      "Prix Goncourt in 1978\t\tNobel Prize in Literature\n",
      "The death of Joseph Lyons\t\tLyons' death in 1939\n",
      "Death of John Curtin\t\tLyons' death in 1939\n",
      "Not directly provided in the text\t\t£23,755\n",
      "Not specified in context\t\tPeter Marc Jacobson\n",
      "Not directly related context\t\tLarry Gagosian\n",
      "The Defence Research\t\tFrom the 1950s to the 1970s\n",
      "During the Cold War era\t\tFrom the 1950s to the 1970s\n",
      "In the 21st century era\t\tFrom the 1950s to the 1970s\n",
      "Ninghai County\t\tNingbo\n",
      "Identical twin sisters\t\tNatalie Albino\n",
      "Nina Sky duo\t\tNatalie Albino\n",
      "There is no information\t\tAugust 30, 2007\n",
      "The identical twin sisters\t\tNatalie Albino\n",
      "Paulsdale, New Jersey\t\tMount Laurel Township\n",
      "Not mentioned in the context\t\tMount Laurel Township\n",
      "The Scottish Parliament Building\t\tScottish Parliament Building\n",
      "Not mentioned in the passage\t\tMount Laurel Township\n",
      "The Scottish Parliament Building\t\tScottish Parliament Building\n",
      "Middleton and Moston\t\tMiddleton\n",
      "Triumph's label is TML Entertainment\t\tTML Entertainment\n",
      "The Allegheny River\t\tDelaware River\n",
      "Egyptian pantheon of gods\t\tancient Egyptian religion\n",
      "The Egyptian pantheon\t\tancient Egyptian religion\n",
      "Lisbon District\t\tLisbon District\n",
      "Vera Barbosa's birth place is\t\tLisbon District\n",
      "Portugal is not mentioned, but Lisbon\t\tLisbon District\n",
      "This information is not provided\t\t12–14 years old\n",
      "Not mentioned in the passage\t\tNazareth\n",
      "Cabo Verde Island nation\t\tcentral Atlantic Ocean\n",
      "Not specified in given context\t\tcentral Atlantic Ocean\n",
      "Located on Sal Island\t\tcentral Atlantic Ocean\n",
      "King Edward's School, Birmingham\t\tUniversity of Glasgow\n",
      "University of Glasgow\t\tUniversity of Glasgow\n",
      "University of Glasgow\t\tUniversity of Glasgow\n",
      "The International Tennis Federation\t\tInternational Tennis Federation\n",
      "International Tennis Federation\t\tInternational Tennis Federation\n",
      "International Tennis Federation\t\tInternational Tennis Federation\n",
      "Starkville is the answer\t\tStarkville\n",
      "French singer Louis Chedid\t\tLouis Chedid\n",
      "Louis Chedid is the father\t\tLouis Chedid\n",
      "Louis Chedid is the father\t\tLouis Chedid\n",
      "Bill Wyman's child\t\tTatum O'Neal\n",
      "Mara Elizabeth Wilson's sibling\t\tLana Wood\n",
      "Natalie Wood's sister\t\tLana Wood\n",
      "Natalie Wood's sister\t\tLana Wood\n",
      "Compaq merged with HP\t\tHewlett Packard\n",
      "Presto Studios merged\t\tHewlett Packard\n",
      "Compaq merged with Hewlett\t\tHewlett Packard\n",
      "Lake Superior\t\tLake Superior\n",
      "Frank Lampard and Arnautovi\t\tMido\n",
      "None of the options mentioned\t\tMido\n",
      "Frank Lampard is not correct\t\tMido\n",
      "The University of Zagreb\t\tCharles University\n",
      "Ernst Mach's employer was unknown\t\tCharles University\n",
      "Charles University in Prague\t\tCharles University\n",
      "Deborah Van Valkenburgh\t\tLisa Rinna\n",
      "Saint Joseph of the Bible\t\tNazareth\n",
      "Andrew Lang died 1912\t\t1912\n",
      "1951 at the University\t\t1912\n",
      "There is no information\t\t1912\n",
      "Guadalupe County shares\t\tBell County\n",
      "Emperor Xiaowu of Jin\t\tYang Xingmi\n",
      "Lynn Haven, Florida\t\tBay County\n",
      "Chiang Hsiao-wen\t\tChiang Hsiao-wu\n",
      "Not mentioned in the context\t\tLiu Yin\n",
      "Chiang Ching-kuo\t\tChiang Hsiao-wu\n",
      "Yeh Raaste Hain Py\t\tYeh Raaste Hain Pyaar Ke\n",
      "Blackmail and Raju Chacha\t\tYeh Raaste Hain Pyaar Ke\n",
      "Yeh Raaste Hain Py\t\tYeh Raaste Hain Pyaar Ke\n",
      "Sire Records\t\tKanine Records\n",
      "I couldn't find any information about the child of\t\tKim Braden\n",
      "Ub Iwerks is not creator\t\tWalt Disney\n",
      "Robert Oppenheimer\t\tRobert Oppenheimer\n",
      "The Socialist Party of America\t\tSocialist Party of America\n",
      "The Socialist Party of America\t\tSocialist Party of America\n",
      "Socialist Party of America\t\tSocialist Party of America\n",
      "Lower Nelson River\t\tHudson's Bay\n",
      "The Nelson River\t\tHudson's Bay\n",
      "The Nelson River\t\tHudson's Bay\n",
      "Helen Pitts Douglass married\t\tHelen Pitts Douglass\n",
      "Helen Pitts Douglass\t\tHelen Pitts Douglass\n",
      "Helen Pitts Douglass was\t\tHelen Pitts Douglass\n",
      "Not mentioned in the text\t\tHelen Mirren\n",
      "Not directly mentioned\t\tPalmiro Togliatti\n",
      "Palmiro Togliatti\t\tPalmiro Togliatti\n",
      "Palmiro Togliatti\t\tPalmiro Togliatti\n",
      "Mu'awiyah introduced postal service\t\t661\n",
      "University of British Columbia\t\tUniversity of British Columbia\n",
      "University of British Columbia\t\tUniversity of British Columbia\n",
      "There is no mention of it\t\tUniversity of British Columbia\n",
      "Sam Houston was named\t\tUnited States National Forest\n",
      "Sarvepalli Radhak\t\tAPJ Abdul Kalam\n",
      "C Rajagopalachari\t\tAPJ Abdul Kalam\n",
      "Nelson Mandela was\t\tAPJ Abdul Kalam\n",
      "A giant panda\t\tpanda\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jczqxp2nf089qng21e2vba75` on : Limit 500000, Used 499497, Requested 1034. Please try again in 1m31.608399999s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': '', 'code': 'rate_limit_exceeded'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/hardik/RAG/rag.ipynb Cell 8\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgaruda.iiit.ac.in/home/hardik/RAG/rag.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=39'>40</a>\u001b[0m     paragraphs \u001b[39m=\u001b[39m paragraphs[:\u001b[39m4000\u001b[39m]    \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgaruda.iiit.ac.in/home/hardik/RAG/rag.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=41'>42</a>\u001b[0m \u001b[39m# prompt = f'''CONTEXT : {paragraphs}.\\nQUESTION : '{complex_question}'.\\nProvide the correct answer to the question in the following format: \"answer: (answer under 5 words)\"'''\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgaruda.iiit.ac.in/home/hardik/RAG/rag.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39m# print(prompt)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgaruda.iiit.ac.in/home/hardik/RAG/rag.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=43'>44</a>\u001b[0m \u001b[39m# response = generate_text(tokenizer, model, prompt, max_new_tokens=10)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgaruda.iiit.ac.in/home/hardik/RAG/rag.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=44'>45</a>\u001b[0m \u001b[39m# model_answer = response[len(prompt):-1].strip()\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bgaruda.iiit.ac.in/home/hardik/RAG/rag.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=47'>48</a>\u001b[0m chat_completion \u001b[39m=\u001b[39m client\u001b[39m.\u001b[39;49mchat\u001b[39m.\u001b[39;49mcompletions\u001b[39m.\u001b[39;49mcreate(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgaruda.iiit.ac.in/home/hardik/RAG/rag.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=48'>49</a>\u001b[0m     messages\u001b[39m=\u001b[39;49m[\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgaruda.iiit.ac.in/home/hardik/RAG/rag.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=49'>50</a>\u001b[0m         {\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgaruda.iiit.ac.in/home/hardik/RAG/rag.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=50'>51</a>\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mrole\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39msystem\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgaruda.iiit.ac.in/home/hardik/RAG/rag.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=51'>52</a>\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mcontent\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mUsing the CONTEXT, give the most accurate answer to the QUESTION in the format `ANSWER : (upto 5 word answer)\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgaruda.iiit.ac.in/home/hardik/RAG/rag.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=52'>53</a>\u001b[0m         },\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgaruda.iiit.ac.in/home/hardik/RAG/rag.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=53'>54</a>\u001b[0m         {\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgaruda.iiit.ac.in/home/hardik/RAG/rag.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=54'>55</a>\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mrole\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39muser\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgaruda.iiit.ac.in/home/hardik/RAG/rag.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=55'>56</a>\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mcontent\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mCONTEXT : \u001b[39;49m\u001b[39m{\u001b[39;49;00mparagraphs\u001b[39m}\u001b[39;49;00m\u001b[39m \u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39mQUESTION : \u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m{\u001b[39;49;00mcomplex_question\u001b[39m}\u001b[39;49;00m\u001b[39m'\u001b[39;49m\u001b[39m. ANSWER :\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgaruda.iiit.ac.in/home/hardik/RAG/rag.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=56'>57</a>\u001b[0m         }\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgaruda.iiit.ac.in/home/hardik/RAG/rag.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=57'>58</a>\u001b[0m     ],\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgaruda.iiit.ac.in/home/hardik/RAG/rag.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=58'>59</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgaruda.iiit.ac.in/home/hardik/RAG/rag.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=59'>60</a>\u001b[0m     \u001b[39m# The language model which will generate the completion.\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgaruda.iiit.ac.in/home/hardik/RAG/rag.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=60'>61</a>\u001b[0m     model\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mllama-3.1-8b-instant\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgaruda.iiit.ac.in/home/hardik/RAG/rag.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=61'>62</a>\u001b[0m     temperature\u001b[39m=\u001b[39;49m\u001b[39m0.4\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgaruda.iiit.ac.in/home/hardik/RAG/rag.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=62'>63</a>\u001b[0m     max_tokens\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgaruda.iiit.ac.in/home/hardik/RAG/rag.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=63'>64</a>\u001b[0m     top_p\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgaruda.iiit.ac.in/home/hardik/RAG/rag.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=64'>65</a>\u001b[0m     stop\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgaruda.iiit.ac.in/home/hardik/RAG/rag.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=65'>66</a>\u001b[0m     stream\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgaruda.iiit.ac.in/home/hardik/RAG/rag.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=66'>67</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgaruda.iiit.ac.in/home/hardik/RAG/rag.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=68'>69</a>\u001b[0m \u001b[39m# Print the completion returned by the LLM.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgaruda.iiit.ac.in/home/hardik/RAG/rag.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=69'>70</a>\u001b[0m model_answer \u001b[39m=\u001b[39m chat_completion\u001b[39m.\u001b[39mchoices[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mmessage\u001b[39m.\u001b[39mcontent\n",
      "File \u001b[0;32m~/venvs/Llama/lib/python3.10/site-packages/groq/resources/chat/completions.py:298\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, parallel_tool_calls, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m    158\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    159\u001b[0m     \u001b[39m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    186\u001b[0m     timeout: \u001b[39mfloat\u001b[39m \u001b[39m|\u001b[39m httpx\u001b[39m.\u001b[39mTimeout \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m|\u001b[39m NotGiven \u001b[39m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    187\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ChatCompletion \u001b[39m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m    188\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[39m    Creates a model response for the given chat conversation.\u001b[39;00m\n\u001b[1;32m    190\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[39m      timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[1;32m    297\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 298\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_post(\n\u001b[1;32m    299\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39m/openai/v1/chat/completions\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    300\u001b[0m         body\u001b[39m=\u001b[39;49mmaybe_transform(\n\u001b[1;32m    301\u001b[0m             {\n\u001b[1;32m    302\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmessages\u001b[39;49m\u001b[39m\"\u001b[39;49m: messages,\n\u001b[1;32m    303\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmodel\u001b[39;49m\u001b[39m\"\u001b[39;49m: model,\n\u001b[1;32m    304\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mfrequency_penalty\u001b[39;49m\u001b[39m\"\u001b[39;49m: frequency_penalty,\n\u001b[1;32m    305\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mfunction_call\u001b[39;49m\u001b[39m\"\u001b[39;49m: function_call,\n\u001b[1;32m    306\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mfunctions\u001b[39;49m\u001b[39m\"\u001b[39;49m: functions,\n\u001b[1;32m    307\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mlogit_bias\u001b[39;49m\u001b[39m\"\u001b[39;49m: logit_bias,\n\u001b[1;32m    308\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mlogprobs\u001b[39;49m\u001b[39m\"\u001b[39;49m: logprobs,\n\u001b[1;32m    309\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmax_tokens\u001b[39;49m\u001b[39m\"\u001b[39;49m: max_tokens,\n\u001b[1;32m    310\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mn\u001b[39;49m\u001b[39m\"\u001b[39;49m: n,\n\u001b[1;32m    311\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mparallel_tool_calls\u001b[39;49m\u001b[39m\"\u001b[39;49m: parallel_tool_calls,\n\u001b[1;32m    312\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mpresence_penalty\u001b[39;49m\u001b[39m\"\u001b[39;49m: presence_penalty,\n\u001b[1;32m    313\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mresponse_format\u001b[39;49m\u001b[39m\"\u001b[39;49m: response_format,\n\u001b[1;32m    314\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mseed\u001b[39;49m\u001b[39m\"\u001b[39;49m: seed,\n\u001b[1;32m    315\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mstop\u001b[39;49m\u001b[39m\"\u001b[39;49m: stop,\n\u001b[1;32m    316\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mstream\u001b[39;49m\u001b[39m\"\u001b[39;49m: stream,\n\u001b[1;32m    317\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtemperature\u001b[39;49m\u001b[39m\"\u001b[39;49m: temperature,\n\u001b[1;32m    318\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtool_choice\u001b[39;49m\u001b[39m\"\u001b[39;49m: tool_choice,\n\u001b[1;32m    319\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtools\u001b[39;49m\u001b[39m\"\u001b[39;49m: tools,\n\u001b[1;32m    320\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtop_logprobs\u001b[39;49m\u001b[39m\"\u001b[39;49m: top_logprobs,\n\u001b[1;32m    321\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtop_p\u001b[39;49m\u001b[39m\"\u001b[39;49m: top_p,\n\u001b[1;32m    322\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39muser\u001b[39;49m\u001b[39m\"\u001b[39;49m: user,\n\u001b[1;32m    323\u001b[0m             },\n\u001b[1;32m    324\u001b[0m             completion_create_params\u001b[39m.\u001b[39;49mCompletionCreateParams,\n\u001b[1;32m    325\u001b[0m         ),\n\u001b[1;32m    326\u001b[0m         options\u001b[39m=\u001b[39;49mmake_request_options(\n\u001b[1;32m    327\u001b[0m             extra_headers\u001b[39m=\u001b[39;49mextra_headers, extra_query\u001b[39m=\u001b[39;49mextra_query, extra_body\u001b[39m=\u001b[39;49mextra_body, timeout\u001b[39m=\u001b[39;49mtimeout\n\u001b[1;32m    328\u001b[0m         ),\n\u001b[1;32m    329\u001b[0m         cast_to\u001b[39m=\u001b[39;49mChatCompletion,\n\u001b[1;32m    330\u001b[0m         stream\u001b[39m=\u001b[39;49mstream \u001b[39mor\u001b[39;49;00m \u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    331\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mStream[ChatCompletionChunk],\n\u001b[1;32m    332\u001b[0m     )\n",
      "File \u001b[0;32m~/venvs/Llama/lib/python3.10/site-packages/groq/_base_client.py:1261\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1247\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpost\u001b[39m(\n\u001b[1;32m   1248\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   1249\u001b[0m     path: \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1256\u001b[0m     stream_cls: \u001b[39mtype\u001b[39m[_StreamT] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1257\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ResponseT \u001b[39m|\u001b[39m _StreamT:\n\u001b[1;32m   1258\u001b[0m     opts \u001b[39m=\u001b[39m FinalRequestOptions\u001b[39m.\u001b[39mconstruct(\n\u001b[1;32m   1259\u001b[0m         method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpost\u001b[39m\u001b[39m\"\u001b[39m, url\u001b[39m=\u001b[39mpath, json_data\u001b[39m=\u001b[39mbody, files\u001b[39m=\u001b[39mto_httpx_files(files), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions\n\u001b[1;32m   1260\u001b[0m     )\n\u001b[0;32m-> 1261\u001b[0m     \u001b[39mreturn\u001b[39;00m cast(ResponseT, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest(cast_to, opts, stream\u001b[39m=\u001b[39;49mstream, stream_cls\u001b[39m=\u001b[39;49mstream_cls))\n",
      "File \u001b[0;32m~/venvs/Llama/lib/python3.10/site-packages/groq/_base_client.py:953\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    950\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    951\u001b[0m     retries_taken \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m--> 953\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(\n\u001b[1;32m    954\u001b[0m     cast_to\u001b[39m=\u001b[39;49mcast_to,\n\u001b[1;32m    955\u001b[0m     options\u001b[39m=\u001b[39;49moptions,\n\u001b[1;32m    956\u001b[0m     stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    957\u001b[0m     stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[1;32m    958\u001b[0m     retries_taken\u001b[39m=\u001b[39;49mretries_taken,\n\u001b[1;32m    959\u001b[0m )\n",
      "File \u001b[0;32m~/venvs/Llama/lib/python3.10/site-packages/groq/_base_client.py:1056\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1053\u001b[0m         err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mread()\n\u001b[1;32m   1055\u001b[0m     log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mRe-raising status error\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1056\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_status_error_from_response(err\u001b[39m.\u001b[39mresponse) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1058\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_response(\n\u001b[1;32m   1059\u001b[0m     cast_to\u001b[39m=\u001b[39mcast_to,\n\u001b[1;32m   1060\u001b[0m     options\u001b[39m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1064\u001b[0m     retries_taken\u001b[39m=\u001b[39mretries_taken,\n\u001b[1;32m   1065\u001b[0m )\n",
      "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jczqxp2nf089qng21e2vba75` on : Limit 500000, Used 499497, Requested 1034. Please try again in 1m31.608399999s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': '', 'code': 'rate_limit_exceeded'}}"
     ]
    }
   ],
   "source": [
    "client = Groq()\n",
    "\n",
    "use_iterative_RAG = True\n",
    "\n",
    "print('Reponse\\t\\tGround Truth')\n",
    "\n",
    "outputs = []\n",
    "responses = []\n",
    "gts = []\n",
    "for i in range(3000):\n",
    "    \n",
    "    with open('RAG_results.json', 'r') as f:\n",
    "        add_to_json = json.load(f)\n",
    "        \n",
    "    if add_to_json.get(f'{i}'):\n",
    "        continue\n",
    "    \n",
    "    paragraphs = val_df[\"paragraphs\"][i]\n",
    "    paragraphs = [p['paragraph_text'] for p in paragraphs]\n",
    "    \n",
    "            \n",
    "    complex_question = val_df[\"question\"][i]\n",
    "    # print(complex_question)\n",
    "    \n",
    "    closest_paragraphs = get_closest_paragraphs(complex_question, paragraphs, top_k=3)\n",
    "    if use_iterative_RAG:\n",
    "        second_closest_paragraphs = [get_closest_paragraphs(cp, paragraphs, top_k=2, second_closest=True) for cp in closest_paragraphs]\n",
    "        second_closest_paragraphs = [item for sublist in second_closest_paragraphs for item in sublist]\n",
    "        # print(\">>>\", second_closest_paragraphs)\n",
    "        closest_paragraphs.extend(second_closest_paragraphs)\n",
    "        closest_paragraphs = list(set(closest_paragraphs))\n",
    "    \n",
    "    paragraphs = \" \".join(closest_paragraphs)\n",
    "    \n",
    "    gt_para_idxs = val_df[\"question_decomposition\"][i]\n",
    "    gt_para_idxs = [gt['paragraph_support_idx'] for gt in gt_para_idxs]\n",
    "    # print(\"gt_para_idxs : \", gt_para_idxs)\n",
    "    \n",
    "    if len(paragraphs) > 4000:\n",
    "        paragraphs = paragraphs[:4000]    \n",
    "    \n",
    "    # prompt = f'''CONTEXT : {paragraphs}.\\nQUESTION : '{complex_question}'.\\nProvide the correct answer to the question in the following format: \"answer: (answer under 5 words)\"'''\n",
    "    # print(prompt)\n",
    "    # response = generate_text(tokenizer, model, prompt, max_new_tokens=10)\n",
    "    # model_answer = response[len(prompt):-1].strip()\n",
    "    \n",
    "\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"Using the CONTEXT, give the most accurate answer to the QUESTION in the format `ANSWER : (upto 5 word answer)\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"CONTEXT : {paragraphs} \\nQUESTION : '{complex_question}'. ANSWER :\",\n",
    "            }\n",
    "        ],\n",
    "\n",
    "        # The language model which will generate the completion.\n",
    "        model=\"llama-3.1-8b-instant\",\n",
    "        temperature=0.4,\n",
    "        max_tokens=10,\n",
    "        top_p=1,\n",
    "        stop=None,\n",
    "        stream=False,\n",
    "    )\n",
    "\n",
    "    # Print the completion returned by the LLM.\n",
    "    model_answer = chat_completion.choices[0].message.content\n",
    "    \n",
    "    model_answer = model_answer.replace(\"ANSWER : \", \"\")\n",
    "    model_answer = model_answer.replace(\"\\n\", \"\")\n",
    "    model_answer = model_answer.replace(\".\", \"\")\n",
    "    model_answer = model_answer.strip()\n",
    "    \n",
    "    gt = val_df[\"answer\"][i].replace(\".\", \"\")\n",
    "    \n",
    "    \n",
    "    add_to_json.update({\n",
    "        i: {\n",
    "            'model_answer': model_answer,\n",
    "            'gt': gt\n",
    "        }\n",
    "    })\n",
    "    \n",
    "    with open('RAG_results.json', 'w') as f:\n",
    "        json.dump(add_to_json, f, indent=4)\n",
    "    \n",
    "    \n",
    "    print(f'{model_answer}\\t\\t{gt}')\n",
    "    \n",
    "    # outputs.append(response)\n",
    "    responses.append(model_answer)\n",
    "    gts.append(gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the gts\n",
    "with open(\"iterative_RAG_outputs.txt\", \"w\") as f:\n",
    "    for response in responses:\n",
    "        f.write(response + \"\\n\")\n",
    "        \n",
    "with open(\"iterative_RAG_gts.txt\", \"w\") as f:\n",
    "    for gt in gts:\n",
    "        f.write(gt + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Miquette Giraudy',\n",
       "  'Mike Medavoy',\n",
       "  'Tamaulipas',\n",
       "  'Cologne',\n",
       "  'Bombardier Inc',\n",
       "  'Fletcher Webster',\n",
       "  \"Marie de' Medici\",\n",
       "  'European integration',\n",
       "  'Time Warner Cable',\n",
       "  'Cabo Delgado Province',\n",
       "  'Lega Pro Prima Divisione',\n",
       "  'Anderson Tapes',\n",
       "  'Knowsley',\n",
       "  'Crockett County',\n",
       "  'Orleans County',\n",
       "  'Academy Award for Best Animated Short Film',\n",
       "  'Blair High School',\n",
       "  'Keith Urban',\n",
       "  'Svante Stensson Sture',\n",
       "  'Randall County'],\n",
       " ['No information provided about spouse',\n",
       "  'Kevin Smith',\n",
       "  'Municipality of Nuevo Laredo',\n",
       "  'There is no mention of Ulrich Walter, so',\n",
       "  'Bombardier Inc',\n",
       "  'Daniel Bremer Juell',\n",
       "  'Anne of Austria',\n",
       "  'Promoting European integration',\n",
       "  'Adelphia Communications Corporation',\n",
       "  'Niassa Province',\n",
       "  'Serie A',\n",
       "  'The Timothy Files',\n",
       "  'Merseyside',\n",
       "  'Texas',\n",
       "  'Holley, New York',\n",
       "  'PEN/Diamonstein-Spielvog',\n",
       "  'Blair International Baccalaureate School',\n",
       "  'Nicole Kidman',\n",
       "  'Svante Turesson Bielke',\n",
       "  'Randall County, Texas'])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gts, responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average ROUGE-1 F1 score:  0.1752380937731973\n",
      "Average ROUGE-2 F1 score:  0.08333333286111111\n",
      "Average ROUGE-L F1 score:  0.1752380937731973\n",
      "Exact matches:  0.05\n",
      "Partial matches:  0.3\n"
     ]
    }
   ],
   "source": [
    "# compare the responses and the ground truths with different metrics in NLP\n",
    "\n",
    "# BLEU score\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from pycocoevalcap.cider.cider import Cider\n",
    "from pycocoevalcap.spice.spice import Spice\n",
    "from rouge import Rouge\n",
    "\n",
    "\n",
    "rouge = Rouge()\n",
    "cider = Cider()\n",
    "spice = Spice()\n",
    "exact_matches = 0\n",
    "num_partial_matches = 0\n",
    "\n",
    "bleu_scores = []\n",
    "cider_scores = []\n",
    "meteor_scores = []\n",
    "rouge_scores = []\n",
    "spice_scores = []\n",
    "for i in range(len(gts)):\n",
    "    # bleu_scores.append(sentence_bleu([gts[i].split()], responses[i].split()))\n",
    "    # cider_scores.append(cider.compute_score(gts[i], responses[i]))\n",
    "    # meteor_scores.append(meteor_score(gts[i], responses[i]))\n",
    "    rouge_scores.append(rouge.get_scores(responses[i], gts[i]))\n",
    "    # spice_scores.append(spice.compute_score(gts[i], responses[i]))\n",
    "    # exact match\n",
    "    if gts[i] == responses[i]:\n",
    "        exact_matches += 1\n",
    "        \n",
    "    words_gt = gts[i].split()\n",
    "    words_response = responses[i].split()\n",
    "    \n",
    "    # partial match by seeing how may words in the response are in the ground truth\n",
    "    num_partial_matches += (len(set(words_gt).intersection(set(words_response)))\n",
    "    \n",
    "     \n",
    "rouge_1_f = 0\n",
    "rouge_2_f = 0\n",
    "rouge_l_f = 0   \n",
    "\n",
    "for i in range(len(rouge_scores)):\n",
    "    rouge_1_f+= rouge_scores[i][0]['rouge-1']['f']\n",
    "    rouge_2_f+= rouge_scores[i][0]['rouge-2']['f']\n",
    "    rouge_l_f+= rouge_scores[i][0]['rouge-l']['f']\n",
    "    \n",
    "rouge_1_f = rouge_1_f/len(rouge_scores)\n",
    "rouge_2_f = rouge_2_f/len(rouge_scores)\n",
    "rouge_l_f = rouge_l_f/len(rouge_scores)\n",
    "\n",
    "print(\"Average ROUGE-1 F1 score: \", rouge_1_f)\n",
    "print(\"Average ROUGE-2 F1 score: \", rouge_2_f)\n",
    "print(\"Average ROUGE-L F1 score: \", rouge_l_f)\n",
    "    \n",
    "        \n",
    "print(\"Exact matches: \", exact_matches/len(gts))\n",
    "print(\"Partial matches: \", num_partial_matches/len(gts))\n",
    "\n",
    "# print(\"BLEU scores: \", bleu_scores)\n",
    "# print(\"CIDEr scores: \", cider_scores)\n",
    "# print(\"METEOR scores: \", meteor_scores)\n",
    "# print(\"ROUGE scores: \", rouge_scores)\n",
    "# print(\"SPICE scores: \", spice_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Llama",
   "language": "python",
   "name": "llama"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
